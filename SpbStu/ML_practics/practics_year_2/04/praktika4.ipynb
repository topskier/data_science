{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y7Ssty1HgqCU"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from scipy.sparse import csr_matrix, lil_matrix\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Im6904hdbbsD"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('sample_data/kz.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "KuLFL2mfFjvC"
      },
      "outputs": [],
      "source": [
        "# if 'event_type' in df.columns:\n",
        "#     df = df[df['event_type'] == 'purchase']\n",
        "df = df.rename(columns={'user_id': 'userId', 'product_id': 'movieId', 'event_time': 'timestamp'})\n",
        "df = df.drop_duplicates(subset=['order_id', 'movieId'])\n",
        "df = df.dropna(subset=['userId', 'movieId'])\n",
        "df['timestamp'] = pd.to_datetime(df['timestamp'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CAexV0reFjvD"
      },
      "source": [
        "<br1>Наивная рекомендация</br1>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FnGc1A_SFjvF",
        "outputId": "637f1f65-d0d6-473f-9755-29e905980dfc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Selected k: 3\n"
          ]
        }
      ],
      "source": [
        "# Определяем k на основе медианного числа товаров повторного клиента\n",
        "repeat_customers = df.groupby('userId').size()\n",
        "k = int(repeat_customers.median())\n",
        "k = max(3, min(k, 10))  # Ограничиваем k от 3 до 10\n",
        "print(f'Selected k: {k}')\n",
        "\n",
        "# Разделяем данные по дате (2020-09-01)\n",
        "train_data = df[df['timestamp'] < '2020-09-01']\n",
        "test_data = df[df['timestamp'] >= '2020-09-01']\n",
        "\n",
        "def naive_popularity_based(train_data, k):\n",
        "    popular_items = train_data['movieId'].value_counts().head(k).index.tolist()\n",
        "    return popular_items\n",
        "\n",
        "# Наивный алгоритм 2: Любимый товар клиента + (k-1) самых популярных\n",
        "def naive_hybrid_based(train_data, k):\n",
        "    # Самые популярные товары\n",
        "    popular_items = train_data['movieId'].value_counts().head(k).index.tolist()\n",
        "\n",
        "    # Любимый товар каждого клиента (самый частый)\n",
        "    user_favorites = train_data.groupby('userId')['movieId'].apply(\n",
        "        lambda x: x.value_counts().index[0] if len(x.value_counts()) > 0 else popular_items[0]\n",
        "    ).to_dict()\n",
        "\n",
        "    recommendations = {}\n",
        "    for user in set(train_data['userId'].unique()) | set(test_data['userId'].unique()):\n",
        "        if user in user_favorites:\n",
        "            favorite = user_favorites[user]\n",
        "            # Убедимся, что favorite не входит в popular_items\n",
        "            user_rec = [favorite] + [item for item in popular_items if item != favorite][:k-1]\n",
        "        else:\n",
        "            # Если пользователя нет в тренировочных данных, используем популярные товары\n",
        "            user_rec = popular_items[:k]\n",
        "        recommendations[user] = user_rec[:k]\n",
        "\n",
        "    return recommendations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "hV215PtcFjvG"
      },
      "outputs": [],
      "source": [
        "def evaluate_recommendations(recommendations, test_data, k):\n",
        "    precisions = []\n",
        "    recalls = []\n",
        "    f1_scores = []\n",
        "\n",
        "    # Группируем тестовые данные по пользователям\n",
        "    test_user_items = test_data.groupby('userId')['movieId'].apply(set).to_dict()\n",
        "\n",
        "    for user, actual_items in test_user_items.items():\n",
        "        if user in recommendations:\n",
        "            recommended_items = set(recommendations[user][:k])\n",
        "        else:\n",
        "            # Если пользователя нет в рекомендациях, пропускаем\n",
        "            continue\n",
        "\n",
        "        if len(recommended_items) == 0:\n",
        "            precision = recall = f1 = 0\n",
        "        else:\n",
        "            # Вычисляем precision и recall\n",
        "            true_positives = len(recommended_items & actual_items)\n",
        "            precision = true_positives / len(recommended_items)\n",
        "            recall = true_positives / len(actual_items) if len(actual_items) > 0 else 0\n",
        "\n",
        "            # Вычисляем F1-score\n",
        "            if precision + recall > 0:\n",
        "                f1 = 2 * (precision * recall) / (precision + recall)\n",
        "            else:\n",
        "                f1 = 0\n",
        "\n",
        "        precisions.append(precision)\n",
        "        recalls.append(recall)\n",
        "        f1_scores.append(f1)\n",
        "\n",
        "    # Усредняем метрики по всем пользователям\n",
        "    avg_precision = np.mean(precisions) if precisions else 0\n",
        "    avg_recall = np.mean(recalls) if recalls else 0\n",
        "    avg_f1 = np.mean(f1_scores) if f1_scores else 0\n",
        "\n",
        "    return avg_precision, avg_recall, avg_f1\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ze1gso_YFjvH",
        "outputId": "b87ac906-c2f1-4273-f4b5-c4ee8dfbbb92"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Popularity Based - Precision: 0.0066, Recall: 0.0068, F1: 0.0052\n",
            "Hybrid Based - Precision: 0.0114, Recall: 0.0155, F1: 0.0107\n"
          ]
        }
      ],
      "source": [
        "popular_items = naive_popularity_based(train_data, k)\n",
        "popular_recommendations = {user: popular_items for user in test_data['userId'].unique()}\n",
        "hybrid_recommendations = naive_hybrid_based(train_data, k)\n",
        "# hybrid_recommendations = naive_hybrid_based(train_data, k)\n",
        "# random_recommendations = naive_random_based(train_data, k)\n",
        "\n",
        "precision_pop, recall_pop, f1_pop  = evaluate_recommendations(popular_recommendations, test_data, 3)\n",
        "# f1_hybrid = evaluate_naive_method(hybrid_recommendations, test_data)\n",
        "# f1_random = evaluate_naive_method(random_recommendations, test_data)\n",
        "precision_hyb, recall_hyb, f1_hyb = evaluate_recommendations(hybrid_recommendations, test_data, k)\n",
        "print(f'Popularity Based - Precision: {precision_pop:.4f}, Recall: {recall_pop:.4f}, F1: {f1_pop:.4f}')\n",
        "print(f'Hybrid Based - Precision: {precision_hyb:.4f}, Recall: {recall_hyb:.4f}, F1: {f1_hyb:.4f}')\n",
        "# print(f'F1-Score Hybrid Based: {f1_hybrid:.4f}')\n",
        "# print(f'F1-Score Random Based: {f1_random:.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SuhvlRA9FjvJ"
      },
      "source": [
        "<br1>Продвинутые методы</br1>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "UkF-aOXbFjvJ"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import f1_score, precision_score, recall_score\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from scipy.sparse import csr_matrix, lil_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "pwEaRYZWFjvK"
      },
      "outputs": [],
      "source": [
        "def create_sparse_matrix(data, user_col='userId', item_col='movieId'):\n",
        "\n",
        "    # Создаем mapping пользователей и товаров\n",
        "    users = sorted(data[user_col].unique())\n",
        "    items = sorted(data[item_col].unique())\n",
        "\n",
        "    user_to_idx = {user: idx for idx, user in enumerate(users)}\n",
        "    item_to_idx = {item: idx for idx, item in enumerate(items)}\n",
        "\n",
        "    # Создаем разреженную матрицу в формате LIL (более эффективно для построения)\n",
        "    matrix = lil_matrix((len(users), len(items)), dtype=np.float32)\n",
        "\n",
        "    for _, row in data.iterrows():\n",
        "        user_idx = user_to_idx[row[user_col]]\n",
        "        item_idx = item_to_idx[row[item_col]]\n",
        "        matrix[user_idx, item_idx] = 1  # Бинарные данные\n",
        "\n",
        "    return matrix.tocsr(), users, items, user_to_idx, item_to_idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M404p8bmFjvL",
        "outputId": "420a640a-00b2-4105-ed82-1ee268f4c90f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Разреженная матрица обучения: (80201, 16636), заполненность: 0.0002%\n",
            "Разреженная матрица теста: (34942, 14976), заполненность: 0.0004%\n"
          ]
        }
      ],
      "source": [
        "train_sparse, train_users, train_items, train_user_map, train_item_map = create_sparse_matrix(train_data)\n",
        "test_sparse, test_users, test_items, test_user_map, test_item_map = create_sparse_matrix(test_data)\n",
        "\n",
        "print(f\"Разреженная матрица обучения: {train_sparse.shape}, заполненность: {train_sparse.nnz / (train_sparse.shape[0] * train_sparse.shape[1]):.4f}%\")\n",
        "print(f\"Разреженная матрица теста: {test_sparse.shape}, заполненность: {test_sparse.nnz / (test_sparse.shape[0] * test_sparse.shape[1]):.4f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0kxx2rhjFjvL"
      },
      "outputs": [],
      "source": [
        "def knn_recommendations(sparse_matrix, k_neighbors=50, k_recommendations=10):\n",
        "    \"\"\"Рекомендации на основе k ближайших соседей\"\"\"\n",
        "    # Используем k-NN для нахождения похожих пользователей\n",
        "    knn = NearestNeighbors(metric='cosine', algorithm='brute', n_neighbors=k_neighbors)\n",
        "    knn.fit(sparse_matrix)\n",
        "\n",
        "    # Получаем расстояния и индексы соседей\n",
        "    distances, indices = knn.kneighbors(sparse_matrix)\n",
        "\n",
        "    # Преобразуем расстояния в веса (близость)\n",
        "    weights = 1 - distances\n",
        "\n",
        "    recommendations = []\n",
        "    for i in range(sparse_matrix.shape[0]):\n",
        "        # Взвешенная сумма оценок соседей\n",
        "        neighbor_weights = weights[i]\n",
        "        neighbor_indices = indices[i]\n",
        "\n",
        "        # Суммируем покупки соседей с весами\n",
        "        weighted_sum = sparse_matrix[neighbor_indices].multiply(neighbor_weights[:, np.newaxis]).sum(axis=0)\n",
        "\n",
        "        # Исключаем товары, которые пользователь уже покупал\n",
        "        user_purchases = sparse_matrix[i].nonzero()[1]\n",
        "        if len(user_purchases) > 0:\n",
        "            weighted_sum[0, user_purchases] = 0\n",
        "\n",
        "        # Получаем топ-k рекомендаций\n",
        "        if weighted_sum.sum() > 0:\n",
        "            top_items = np.argsort(weighted_sum.A1)[-k_recommendations:][::-1]\n",
        "        else:\n",
        "            # Если нет рекомендаций, используем популярные товары\n",
        "            item_popularity = sparse_matrix.sum(axis=0).A1\n",
        "            top_items = np.argsort(item_popularity)[-k_recommendations:][::-1]\n",
        "\n",
        "        recommendations.append(top_items)\n",
        "\n",
        "    return recommendations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "tK5-OmCJFjvM"
      },
      "outputs": [],
      "source": [
        "def simple_collaborative_filtering(train_sparse, k_recommendations=10):\n",
        "        \"\"\"Упрощенная коллаборативная фильтрация на основе общих покупок\"\"\"\n",
        "        # Вычисляем similarity как количество общих покупок\n",
        "        similarity = train_sparse.dot(train_sparse.T)\n",
        "\n",
        "        recommendations = []\n",
        "        for i in range(train_sparse.shape[0]):\n",
        "            # Находим наиболее похожих пользователей\n",
        "            user_similarities = similarity[i].toarray().flatten()\n",
        "\n",
        "            # Исключаем самого пользователя\n",
        "            user_similarities[i] = 0\n",
        "\n",
        "            if user_similarities.sum() > 0:\n",
        "                # Взвешенная сумма покупок похожих пользователей\n",
        "                similar_users = user_similarities.argsort()[-50:][::-1]  # Топ-50 похожих\n",
        "                weights = user_similarities[similar_users]\n",
        "\n",
        "                # ИСПРАВЛЕНИЕ: убираем .A1, так как результат уже массив\n",
        "                # Используем toarray().flatten() для преобразования разреженной матрицы в массив\n",
        "                weighted_sum = train_sparse[similar_users].T.dot(weights).flatten()\n",
        "\n",
        "                # Исключаем уже купленные товары\n",
        "                user_purchases = train_sparse[i].nonzero()[1]\n",
        "                if len(user_purchases) > 0:\n",
        "                    weighted_sum[user_purchases] = 0\n",
        "\n",
        "                # Получаем рекомендации\n",
        "                if weighted_sum.sum() > 0:\n",
        "                    top_items = np.argsort(weighted_sum)[-k_recommendations:][::-1]\n",
        "                else:\n",
        "                    item_popularity = train_sparse.sum(axis=0).flatten()\n",
        "                    top_items = np.argsort(item_popularity)[-k_recommendations:][::-1]\n",
        "            else:\n",
        "                # Если нет похожих пользователей, используем популярные товары\n",
        "                item_popularity = train_sparse.sum(axis=0).flatten()\n",
        "                top_items = np.argsort(item_popularity)[-k_recommendations:][::-1]\n",
        "\n",
        "            recommendations.append(top_items)\n",
        "\n",
        "        return recommendations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "VhJLk62aFjvN"
      },
      "outputs": [],
      "source": [
        "def evaluate_advanced_method(recommendations, test_sparse, item_map, k):\n",
        "\n",
        "    precisions = []\n",
        "    recalls = []\n",
        "    f1_scores = []\n",
        "\n",
        "    for i in range(test_sparse.shape[0]):\n",
        "        # Реальные покупки пользователя в тесте\n",
        "        actual_items = set(test_sparse[i].nonzero()[1])\n",
        "\n",
        "        if i < len(recommendations):\n",
        "            recommended_items = set(recommendations[i][:k])\n",
        "        else:\n",
        "            recommended_items = set()\n",
        "\n",
        "        if len(recommended_items) == 0:\n",
        "            precision = recall = f1 = 0\n",
        "        else:\n",
        "            true_positives = len(recommended_items & actual_items)\n",
        "            precision = true_positives / len(recommended_items)\n",
        "            recall = true_positives / len(actual_items) if len(actual_items) > 0 else 0\n",
        "\n",
        "            if precision + recall > 0:\n",
        "                f1 = 2 * (precision * recall) / (precision + recall)\n",
        "            else:\n",
        "                f1 = 0\n",
        "\n",
        "        precisions.append(precision)\n",
        "        recalls.append(recall)\n",
        "        f1_scores.append(f1)\n",
        "\n",
        "    avg_precision = np.mean(precisions) if precisions else 0\n",
        "    avg_recall = np.mean(recalls) if recalls else 0\n",
        "    avg_f1 = np.mean(f1_scores) if f1_scores else 0\n",
        "\n",
        "    return avg_precision, avg_recall, avg_f1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Kmf5NmKFjvN",
        "outputId": "471d7eae-5a27-4743-fcbf-5fff18e51b70"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'knn_recommendations' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[14], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m knn_recs \u001b[38;5;241m=\u001b[39m \u001b[43mknn_recommendations\u001b[49m(train_sparse, k_recommendations\u001b[38;5;241m=\u001b[39mk)\n\u001b[0;32m      2\u001b[0m precision_knn, recall_knn, f1_knn \u001b[38;5;241m=\u001b[39m evaluate_advanced_method(knn_recs, test_sparse, train_item_map, k)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m f1 - k-NN: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf1_knn\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[1;31mNameError\u001b[0m: name 'knn_recommendations' is not defined"
          ]
        }
      ],
      "source": [
        "knn_recs = knn_recommendations(train_sparse, k_recommendations=k)\n",
        "precision_knn, recall_knn, f1_knn = evaluate_advanced_method(knn_recs, test_sparse, train_item_map, k)\n",
        "print(f\" f1 - k-NN: {f1_knn:.4f}\")\n",
        "print(f\" prec - k-NN: {precision_knn:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zT2_9d_0FjvO"
      },
      "outputs": [],
      "source": [
        "cf_recs = simple_collaborative_filtering(train_sparse, k_recommendations=k)\n",
        "precision_cf, recall_cf, f1_cf = evaluate_advanced_method(cf_recs, test_sparse, train_item_map, k)\n",
        "print(f\" f1 - CF: {f1_cf:.4f}\")\n",
        "print(f\" prec - CF: {precision_cf:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "D6OZOlo9Zy5Y",
        "outputId": "218aa9bd-929f-4b8f-8ee7-b1ca96513b8a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting numpy<2\n",
            "  Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/61.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m52.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.26.4\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "0a7dcb028b1f43e89054aeb94fd733fa",
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "!pip install \"numpy<2\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TDnYeif8aBXx",
        "outputId": "ce25df2b-5435-4db6-c7b0-36055b858019"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting surprise\n",
            "  Downloading surprise-0.1-py2.py3-none-any.whl.metadata (327 bytes)\n",
            "Collecting scikit-surprise (from surprise)\n",
            "  Downloading scikit_surprise-1.1.4.tar.gz (154 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/154.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.4/154.4 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-surprise->surprise) (1.5.2)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.12/dist-packages (from scikit-surprise->surprise) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-surprise->surprise) (1.16.2)\n",
            "Downloading surprise-0.1-py2.py3-none-any.whl (1.8 kB)\n",
            "Building wheels for collected packages: scikit-surprise\n",
            "  Building wheel for scikit-surprise (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for scikit-surprise: filename=scikit_surprise-1.1.4-cp312-cp312-linux_x86_64.whl size=2611313 sha256=932f16851c155922ce5b9bd8dcb8e26878c2ad66a31ed748f9e3eb8223689178\n",
            "  Stored in directory: /root/.cache/pip/wheels/75/fa/bc/739bc2cb1fbaab6061854e6cfbb81a0ae52c92a502a7fa454b\n",
            "Successfully built scikit-surprise\n",
            "Installing collected packages: scikit-surprise, surprise\n",
            "Successfully installed scikit-surprise-1.1.4 surprise-0.1\n"
          ]
        }
      ],
      "source": [
        "!pip install surprise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "nyWZNzrvFjvO"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Установим необходимые библиотеки\n",
        "try:\n",
        "    import surprise\n",
        "except ImportError:\n",
        "    !pip install scikit-surprise\n",
        "    import surprise\n",
        "\n",
        "from surprise import Dataset, Reader, SVD, KNNBasic, accuracy\n",
        "from surprise.model_selection import train_test_split\n",
        "\n",
        "# Явно импортируем numpy и инициализируем для C-расширений\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AmtZlzBeFjvO",
        "outputId": "7fa0af82-a647-4763-ef38-f9d0a29a1b57"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Surprise train size: 321824\n",
            "Surprise test size: 241671\n"
          ]
        }
      ],
      "source": [
        "train_surprise_df = train_data[['userId', 'movieId']].copy()\n",
        "train_surprise_df['rating'] = 1\n",
        "\n",
        "test_surprise_df = test_data[['userId', 'movieId']].copy()\n",
        "test_surprise_df['rating'] = 1\n",
        "\n",
        "print(f\"Surprise train size: {len(train_surprise_df)}\")\n",
        "print(f\"Surprise test size: {len(test_surprise_df)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LUsCip-8Fxd2",
        "outputId": "f6652761-58bd-4909-c4b5-095293176c93"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Обучение модели SVD...\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<surprise.prediction_algorithms.matrix_factorization.SVD at 0x7fa8e9a6e720>"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "reader = Reader(rating_scale=(1, 1))  # у нас все рейтинги 1\n",
        "\n",
        "# Создаем dataset из train_surprise_df\n",
        "data = Dataset.load_from_df(train_surprise_df[['userId', 'movieId', 'rating']], reader)\n",
        "\n",
        "# Разделяем на train и test внутри surprise\n",
        "trainset = data.build_full_trainset()\n",
        "\n",
        "# Обучаем модель SVD (как в примере с Kaggle)\n",
        "print(\"Обучение модели SVD...\")\n",
        "algo_svd = SVD()\n",
        "algo_svd.fit(trainset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "vTYj8k6BcXuT"
      },
      "outputs": [],
      "source": [
        "from surprise import Dataset, Reader, SVD\n",
        "from surprise.model_selection import train_test_split\n",
        "from surprise import accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "dWVYCegbc_zr"
      },
      "outputs": [],
      "source": [
        "def get_surprise_recommendations(algo, test_users, trainset, all_items, k=10):\n",
        "    \"\"\"Получает рекомендации с помощью алгоритма surprise\"\"\"\n",
        "    recommendations = {}\n",
        "\n",
        "    for user in test_users:\n",
        "        # Получаем товары, которые пользователь уже покупал\n",
        "        try:\n",
        "            # Внутренний ID пользователя в trainset\n",
        "            user_inner_id = trainset.to_inner_uid(user)\n",
        "            # Товары, которые пользователь уже оценил (покупал)\n",
        "            user_items = set([trainset.to_raw_iid(iid) for iid in trainset.ur[user_inner_id]])\n",
        "        except ValueError:\n",
        "            # Если пользователя нет в trainset\n",
        "            user_items = set()\n",
        "\n",
        "        # Предсказываем рейтинг для всех товаров, которые пользователь не покупал\n",
        "        user_predictions = []\n",
        "        for item in all_items:\n",
        "            if item in user_items:\n",
        "                continue  # Пропускаем уже купленные товары\n",
        "\n",
        "            try:\n",
        "                pred = algo.predict(user, item)\n",
        "                user_predictions.append((item, pred.est))\n",
        "            except Exception:\n",
        "                continue\n",
        "\n",
        "        # Сортируем по предсказанному рейтингу и берем топ-k\n",
        "        user_predictions.sort(key=lambda x: x[1], reverse=True)\n",
        "        top_k_items = [item for item, _ in user_predictions[:k]]\n",
        "        recommendations[user] = top_k_items\n",
        "\n",
        "    return recommendations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CXI5iYTcF7ot",
        "outputId": "4b1a093a-041b-45cc-b599-8b02e33e60ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Всего товаров: 20964, пользователей в тесте: 34942\n",
            "Получение рекомендаций SVD...\n"
          ]
        }
      ],
      "source": [
        "# Получаем список всех товаров\n",
        "all_items = set(train_surprise_df['movieId'].unique()) | set(test_surprise_df['movieId'].unique())\n",
        "test_users = test_surprise_df['userId'].unique()\n",
        "\n",
        "print(f\"Всего товаров: {len(all_items)}, пользователей в тесте: {len(test_users)}\")\n",
        "\n",
        "# Получаем рекомендации с помощью SVD\n",
        "print(\"Получение рекомендаций SVD...\")\n",
        "svd_recommendations = get_surprise_recommendations(algo_svd, test_users, trainset, all_items, k)\n",
        "\n",
        "\n",
        "\n",
        "# Оцениваем рекомендации для surprise\n",
        "precision_svd, recall_svd, f1_svd = evaluate_recommendations(svd_recommendations, test_data, k)\n",
        "\n",
        "\n",
        "print(f'Surprise SVD - Precision: {precision_svd:.4f}, Recall: {recall_svd:.4f}, F1: {f1_svd:.4f}')\n",
        "\n",
        "\n",
        "# Альтернативный подход: использование тестового набора для оценки RMSE\n",
        "# Создаем тестовый набор для surprise\n",
        "testset = [(uid, iid, 1) for (uid, iid) in zip(test_surprise_df['userId'], test_surprise_df['movieId'])]\n",
        "\n",
        "# Предсказания для тестового набора\n",
        "svd_predictions = algo_svd.test(testset)\n",
        "\n",
        "\n",
        "# Оценка RMSE\n",
        "svd_rmse = accuracy.rmse(svd_predictions, verbose=False)\n",
        "\n",
        "\n",
        "print(f'Surprise SVD RMSE: {svd_rmse:.4f}')\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv (3.12.6)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
